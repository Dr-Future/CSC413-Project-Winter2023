{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Future/CSC413-Project-Winter2023/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and util code"
      ],
      "metadata": {
        "id": "PIom0F8-l0eG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "umvDT8wS_zqI"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For KNN and logistic regression\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "FaBUgElTsjLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self"
      ],
      "metadata": {
        "id": "KCa6tjk9t8uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN-BpYZHFdUZ",
        "outputId": "b0b04007-58ac-42d3-ba2d-2f20b47762b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Laundry"
      ],
      "metadata": {
        "id": "dkklFw7sl42z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_newlines(string):\n",
        "    # Remove '\\r', '\\n', and '\\t' characters using regex\n",
        "    return re.sub(r'[\\r\\n\\t]', '', string)"
      ],
      "metadata": {
        "id": "E7zeTLbbSEfK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_digit(s):\n",
        "  num_str = \"\"\n",
        "  for c in s:\n",
        "      if c.isdigit():\n",
        "          num_str += c\n",
        "\n",
        "  return int(num_str)\n"
      ],
      "metadata": {
        "id": "qMQlt40GWyWt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import chardet\n",
        "\n",
        "#load_data\n",
        "\n",
        "folder_path = '/content/gdrive/My Drive/CSC413 project/test_data' # Replace with the path to your folder containing subfolders with text files\n",
        "\n",
        "# Define a dictionary to store the file contents\n",
        "data_sol = {} #(key=id, value = string)\n",
        "data_ast = {} #(key=id, value = string)\n",
        "vulnerability_classes = {} #(key=id, value = arrayofvoltypes)\n",
        "vulnerability_class_names = []\n",
        "\n",
        "\n",
        "# Loop over each subfolder, and then each file in each subfolder\n",
        "for subfolder_name in os.listdir(folder_path):\n",
        "  subfolder_path = os.path.join(folder_path, subfolder_name)\n",
        "  if os.path.isdir(subfolder_path):\n",
        "    vulnerability_class_names.append(subfolder_name)\n",
        "    for file_name in os.listdir(subfolder_path):\n",
        "      file_path = os.path.join(subfolder_path, file_name)\n",
        "      if os.path.isfile(file_path) and file_name.endswith('.sol'):\n",
        "        file_id = get_digit(file_name)\n",
        "        if file_id in vulnerability_classes:\n",
        "          vulnerability_classes[file_id].append(subfolder_name)\n",
        "        else:\n",
        "          vulnerability_classes[file_id] = [subfolder_name]\n",
        "        if not(file_id in data_sol):\n",
        "          with open(file_path, 'r') as f:\n",
        "            contents = f.read()\n",
        "            data_sol[file_id] = remove_newlines(contents)\n",
        "      if os.path.isdir(file_path):\n",
        "        for ast_file_name in os.listdir(file_path):\n",
        "           ast_file_path = os.path.join(file_path, ast_file_name)\n",
        "           ast_file_id = get_digit(ast_file_name)\n",
        "           if os.path.isfile(ast_file_path) and ast_file_name.endswith('.json') and  not(ast_file_id in data_ast):\n",
        "             with open(ast_file_path, 'rb') as f:\n",
        "              raw_data = f.read()\n",
        "              encoding = chardet.detect(raw_data)['encoding']\n",
        "              contents = raw_data.decode(encoding)\n",
        "              data_ast[ast_file_id] = remove_newlines(contents)\n",
        "\n",
        "              folder_path = '/content/gdrive/My Drive/CSC413 project/test_data' # Replace with the path to your folder containing subfolders with text files\n",
        "\n",
        "# # Define a dictionary to store the file contents\n",
        "# data_sol = {}\n",
        "# data_ast = {}\n",
        "# vulnerability_classes = []\n",
        "\n",
        "\n",
        "# # Loop over each subfolder, and then each file in each subfolder\n",
        "# for subfolder_name in os.listdir(folder_path):\n",
        "#   subfolder_path = os.path.join(folder_path, subfolder_name)\n",
        "#   if os.path.isdir(subfolder_path):\n",
        "#     data_sol[subfolder_name] = []\n",
        "#     vulnerability_classes.append(subfolder_name)\n",
        "#     for file_name in os.listdir(subfolder_path):\n",
        "#       file_path = os.path.join(subfolder_path, file_name)\n",
        "#       if os.path.isfile(file_path) and file_name.endswith('.sol'):\n",
        "#         with open(file_path, 'r') as f:\n",
        "#           contents = f.read()\n",
        "#           data_sol[subfolder_name].append(contents)\n",
        "#       if os.path.isdir(file_path):\n",
        "#         data_ast[subfolder_name] = []\n",
        "#         for ast_file_name in os.listdir(file_path):\n",
        "#            ast_file_path = os.path.join(file_path, ast_file_name)\n",
        "#            if os.path.isfile(ast_file_path) and ast_file_name.endswith('.json'):\n",
        "#              with open(ast_file_path, 'rb') as f:\n",
        "#               raw_data = f.read()\n",
        "#               encoding = chardet.detect(raw_data)['encoding']\n",
        "#               contents = raw_data.decode(encoding)\n",
        "#               data_ast[subfolder_name].append(remove_newlines(contents))\n",
        "\n",
        "          \n",
        "\n"
      ],
      "metadata": {
        "id": "z-qkrlWcBUZb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vulnerability_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry53sT2BFz51",
        "outputId": "fb7db220-b189-4ef8-90f1-b6ea154efc92"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ['block_number_dependency_BN'],\n",
              " 84: ['block_number_dependency_BN'],\n",
              " 103: ['block_number_dependency_BN'],\n",
              " 112: ['block_number_dependency_BN'],\n",
              " 129: ['block_number_dependency_BN'],\n",
              " 135: ['block_number_dependency_BN'],\n",
              " 3521: ['dangerous_delegatecall_DE', 'ether_frozen_EF'],\n",
              " 5854: ['dangerous_delegatecall_DE'],\n",
              " 10708: ['dangerous_delegatecall_DE'],\n",
              " 2004: ['dangerous_delegatecall_DE', 'ether_frozen_EF'],\n",
              " 4617: ['dangerous_delegatecall_DE'],\n",
              " 3972: ['dangerous_delegatecall_DE'],\n",
              " 43311: ['ether_frozen_EF'],\n",
              " 10726: ['ether_frozen_EF'],\n",
              " 44311: ['ether_frozen_EF']}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vulnerability_class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC7jafyBefer",
        "outputId": "d9368ec1-99b9-4dbf-cd26-f6d2419b35a4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['block_number_dependency_BN', 'dangerous_delegatecall_DE', 'ether_frozen_EF']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vulnerability_vector = {id: [1 if class_name in vulnerability_classes[id] else 0 for class_name in vulnerability_class_names] for id in data_sol}\n",
        "vulnerability_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhrmivsUe2nV",
        "outputId": "a92c0ba0-922d-4f87-c288-675ecfc5a59a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [1, 0, 0],\n",
              " 84: [1, 0, 0],\n",
              " 103: [1, 0, 0],\n",
              " 112: [1, 0, 0],\n",
              " 129: [1, 0, 0],\n",
              " 135: [1, 0, 0],\n",
              " 3521: [0, 1, 1],\n",
              " 5854: [0, 1, 0],\n",
              " 10708: [0, 1, 0],\n",
              " 2004: [0, 1, 1],\n",
              " 4617: [0, 1, 0],\n",
              " 3972: [0, 1, 0],\n",
              " 43311: [0, 0, 1],\n",
              " 10726: [0, 0, 1],\n",
              " 44311: [0, 0, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data here\n",
        "# Maybe write a function to return processed data\n",
        "x_train = #...\n",
        "y_train = #...\n",
        "x_test = #...\n",
        "y_test = #..."
      ],
      "metadata": {
        "id": "0ZS41iSMl8c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Nearest Neighbour"
      ],
      "metadata": {
        "id": "JcvCIcl9mObs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_knn(k: int):\n",
        "  # Load the Iris dataset (toy data for debuging only)\n",
        "  iris = datasets.load_iris()\n",
        "  x, y = iris.data, iris.target\n",
        "\n",
        "  # Split the dataset into training and testing sets\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Create a KNN classifier with k neighbors\n",
        "  knn = KNeighborsClassifier(n_neighbors = k)\n",
        "\n",
        "  # Train the classifier with the training data\n",
        "  knn.fit(x_train, y_train)\n",
        "\n",
        "  # Make predictions on the training data\n",
        "  y_train_pred = knn.predict(x_train)\n",
        "\n",
        "  # Evaluate the model's performance on the training set\n",
        "  print(\"Performance on the training set:\")\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_train, y_train_pred))\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "  # Make predictions on the testing data\n",
        "  y_test_pred = knn.predict(x_test)\n",
        "\n",
        "  # Evaluate the model's performance on the testing set\n",
        "  print(\"\\nPerformance on the testing set:\")\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_test, y_test_pred))\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "2kH8eQiymSzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_knn(k = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mGGw6OUtXcu",
        "outputId": "9489b1c4-af91-4705-c754-dc3d4baca115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on the training set:\n",
            "Confusion Matrix:\n",
            "[[40  0  0]\n",
            " [ 0 38  3]\n",
            " [ 0  3 36]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        40\n",
            "           1       0.93      0.93      0.93        41\n",
            "           2       0.92      0.92      0.92        39\n",
            "\n",
            "    accuracy                           0.95       120\n",
            "   macro avg       0.95      0.95      0.95       120\n",
            "weighted avg       0.95      0.95      0.95       120\n",
            "\n",
            "\n",
            "Performance on the testing set:\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "l2SqHCgPotY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SGD():\n",
        "  # Load the Iris dataset (toy data for debuging only)\n",
        "  iris = datasets.load_iris()\n",
        "  x, y = iris.data, iris.target\n",
        "\n",
        "  # Split the dataset into training and testing sets\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Create an SGD classifier\n",
        "  sgd_clf = SGDClassifier()\n",
        "\n",
        "  # Train the model with the training data\n",
        "  sgd_clf.fit(x_train, y_train)\n",
        "\n",
        "  # Make predictions on the training data\n",
        "  y_train_pred = sgd_clf.predict(x_train)\n",
        "\n",
        "  # Evaluate the model's performance on the training set\n",
        "  print(\"Performance on the training set:\")\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_train, y_train_pred))\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "  # Make predictions on the testing data\n",
        "  y_test_pred = sgd_clf.predict(x_test)\n",
        "\n",
        "  # Evaluate the model's performance on the testing set\n",
        "  print(\"\\nPerformance on the testing set:\")\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_test, y_test_pred))\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "pb5kKaKsovml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_SGD()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_RL9ZcuyY2g",
        "outputId": "dfbe71ca-6eaf-49f9-e15f-f8c1e480cc17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on the training set:\n",
            "Confusion Matrix:\n",
            "[[40  0  0]\n",
            " [31  0 10]\n",
            " [ 0  0 39]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72        40\n",
            "           1       0.00      0.00      0.00        41\n",
            "           2       0.80      1.00      0.89        39\n",
            "\n",
            "    accuracy                           0.66       120\n",
            "   macro avg       0.45      0.67      0.54       120\n",
            "weighted avg       0.45      0.66      0.53       120\n",
            "\n",
            "\n",
            "Performance on the testing set:\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 5  0  4]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80        10\n",
            "           1       0.00      0.00      0.00         9\n",
            "           2       0.73      1.00      0.85        11\n",
            "\n",
            "    accuracy                           0.70        30\n",
            "   macro avg       0.47      0.67      0.55        30\n",
            "weighted avg       0.49      0.70      0.58        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "risNUb2q31yI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility code"
      ],
      "metadata": {
        "id": "VBtjiaRa5HEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel(\"Epochs\", fontsize=16)\n",
        "    plt.ylabel(\"Loss\", fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "-7lAMduY5Gt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluating"
      ],
      "metadata": {
        "id": "-XchBR-34ZZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in input_strings\n",
        "        ]\n",
        "        target_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in target_strings\n",
        "        ]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = (\n",
        "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
        "            )  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat(\n",
        "                [decoder_input, targets[:, 0:-1]], dim=1\n",
        "            )  # Gets decoder inputs by shifting the targets to the right\n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(\n",
        "                decoder_inputs, encoder_annotations, decoder_hidden\n",
        "            )\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute gradients\n",
        "                loss.backward()\n",
        "                # Update the parameters of the encoder and decoder\n",
        "                optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
        "\n",
        "        train_loss = compute_loss(\n",
        "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
        "        )\n",
        "        val_loss = compute_loss(\n",
        "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
        "        )\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\n",
        "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
        "                    opts.early_stopping_patience\n",
        "                )\n",
        "            )\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        print(\n",
        "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f}\".format(\n",
        "                epoch, mean_train_loss, mean_val_loss\n",
        "            )\n",
        "        )\n",
        "\n",
        "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = TransformerEncoder(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=opts.hidden_size,\n",
        "        num_layers=opts.num_transformer_layers,\n",
        "        opts=opts,\n",
        "    )\n",
        "\n",
        "    decoder = TransformerDecoder(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=opts.hidden_size,\n",
        "        num_layers=opts.num_transformer_layers,\n",
        "    )\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(\n",
        "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting early from training.\")\n",
        "        return encoder, decoder, losses\n",
        "\n",
        "    return encoder, decoder, losses"
      ],
      "metadata": {
        "id": "5nBTX4ZD33Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.Q = nn.Linear(d_model, d_model)\n",
        "        self.K = nn.Linear(d_model, d_model)\n",
        "        self.V = nn.Linear(d_model, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)    # Changed from dim=2 to dim=1\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        query = self.Q(query).view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        key = self.K(key).view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        value = self.V(value).view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "\n",
        "        query = query.permute(0, 2, 1, 3)\n",
        "        key = key.permute(0, 2, 1, 3)\n",
        "        value = value.permute(0, 2, 1, 3)\n",
        "\n",
        "        energy = torch.matmul(query, key.permute(0, 1, 3, 2)) / self.head_dim**0.5\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        out = torch.matmul(attention, value).permute(0, 2, 1, 3).contiguous()\n",
        "        out = out.view(batch_size, -1, self.d_model)\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ToYTrafVA0Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
        "                annotations, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer.\n",
        "        return annotations, None\n",
        "        # return annotations, None, None\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ],
      "metadata": {
        "id": "Fb9vkD36BU9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}